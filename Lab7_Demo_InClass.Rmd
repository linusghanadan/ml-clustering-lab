---
title: "Clustering Lab"
author: "Linus Ghanadan"
date: "2024-02-29"
output: html_document
---

## Background

Now it's your turn to partition a dataset.  For this lab, we'll use data from Roberts et al. 2008 on bio-contaminants in Sydney Australia's Port Jackson Bay.  The data are measurements of metal content in two types of co-occurring algae at 10 sample sites around the bay.

## Setup & import data

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, message = FALSE, warning = FALSE)
```

```{r}
# set seed
set.seed(123)
```

```{r}
# load packages
library(tidyverse) 
library(tidymodels)
library(cluster) # cluster analysis
library(factoextra) # cluster visualization
library(RColorBrewer)
```

```{r data}
# load data
metals_df <- readr::read_csv(here::here("data/harbour_metals.csv"))

# select only columns for pollutant variables
metals_df <- metals_df[, 4:8] 
```

## K-means clustering

#### Find optimal k value & build model

```{r}
# find optimal number of clusters using elbow method
fviz_nbclust(metals_df, kmeans, method = "wss")
```

The elbow method indicates that k=3 is the ideal number of clusters, as at this point, we see a very significant drop in the Total Within Sum of Square compared to k=2, followed by a very insignificant drop at k=4.

```{r}
# build k-means cluster model with optimal number of clusters (3)
kmeans_model <- kmeans(metals_df, centers = 3, nstart = 25)
```

#### Inspect clustering model

```{r}
# plot the clusters
fviz_cluster(kmeans_result, geom = "point", data = metals_df) + ggtitle("k=3")
```

When we connect the outermost points in each cluster, we see that there is overlap between the far right side of cluster 2 and the far left side of cluster 3. The points near this boundary might have been assigned this way just because of the other nearby points that belonged to each cluster, or this could indicate that we should have chosen a larger number of clusters.

```{r}
# inspect model object
kmeans_model
```

Looking at the specific values, 'size', which give the number of points allocated to each of the three clusters, stands out because cluster 2 and cluster 3 both have more than double the number of points as cluster 1. In addition, our 'withinss' values, which give the within-cluster variation for each of the three clusters, stands out because cluster 3 has over five times more variation than cluster 1 and over three times more variation than cluster 2.

## Hierarchical clustering

#### Calculate Euclidean distance matrix

```{r}
# calculate distance matrix
dist_matrix <- dist(metals_df, method = "euclidean")
```

Each value in this matrix tells us the Euclidean distance between a set of two points in our data. There are 1770 rows in the table, one for each unique set of points.

3. Then apply hierarchical clustering with hclust().

```{r}
# 3. Apply hierarchical clustering
hc_result <- hclust(dist_matrix)
```


4. Now plot the clustering object. You can use something of the form plot(as.dendrogram()).  Or you can check out the cool visual options here: https://rpubs.com/gaston/dendrograms

```{r}

# 4. Plot the clustering object
plot(as.dendrogram(hc_result), main = "Hierarchical Clustering Dendrogram")

```

How does the plot look? Do you see any outliers?  How can you tell?  

[The plot looks as expected. 51 is clearly an outlier point because starting from the bottom of the dendrogram and moving up, it is by far the last point to be assigned to a cluster that includes any other points besides itself, and when this assignment does occur, the point is about 80 distance units away from the centroid of the cluster it is assigned to. Comparatively, the point with the next highest distance away from the centroid of its initial assignment to a cluster containing more than just itself is 15, at about 40 distance units.]{style="color:navy;"}
