---
title: "Clustering Lab"
author: "Linus Ghanadan"
date: "2024-02-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, message = FALSE, warning = FALSE)
```


```{r}
library(tidyverse) 
library(cluster) #cluster analysis
library(factoextra) #cluster visualization
library(tidymodels) #simulation 
library(readr) #read data
library(RColorBrewer)# Color palettes

```

We'll start off with some simulated data that has a structure that is amenable to clustering analysis.

```{r init_sim}
#Set the parameters of our simulated data
set.seed(101)
```

```{r}
# use this to generate all random space points
# each point starts with these centers
# in future chunks we will add noise to these grouping identies
cluster <- factor(1:3)
num_points <- c(100, 150, 50)
x1 = c(5,0,-3)
x2 = c(-1,1,-2)

cents <- tibble(
  cluster = factor(1:3),
  num_points = c(100, 150, 50),
  x1 = c(5,0, -3),
  x2 = c(-1, 1, -2)
)
```

```{r}
#Simulate the data by passing n and mean to rnorm using map2()
labelled_points <-
  cents %>%  mutate(
    x1 = map2(num_points, x1, rnorm),
    x2 = map2(num_points, x2, rnorm),
  ) %>%
  select(-num_points) %>%
  unnest(cols = c(x1, x2))
```

```{r}
ggplot(labelled_points,
       aes(x1, x2, color = cluster)) +
  geom_point(alpha = 0.4)
```


```{r kmeans}
points <-
  labelled_points %>%
  select(-cluster)

kclust <- kmeans(points, centers = 3, n = 1)
kclust
```

```{r syst_k}
#now let's try a systematic method for setting k
kclusts <-
  tibble(k=1:9) %>% 
  mutate(
    kclust = map(k, ~kmeans(points, .x)),
    augmented = map(kclust, augment, points)
  )
kclusts
```

```{r assign}
#append cluster assignment to tibble
assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented))
assignments
```

```{r plot_9_clust}
#Plot each model 
p1 <- ggplot(assignments, aes(x=x1, y=x2)) +
  geom_point(aes(color = .cluster), alpha = 0.8) +
  scale_color_brewer(palette = "Set1") +
  facet_wrap(~k)

p1

```

```{r elbow}
#Use a clustering function from {factoextra} to plot  total WSSs
fviz_nbclust(points, kmeans, "wss")
```


```{r more_fviz}
#Another plotting method
k3 <- kmeans(points, centers = 3, nstart = 25)

p3 <- fviz_cluster(k3, geom = "point", data = points) + ggtitle("k=3")

p3
```


In-class assignment!

Now it's your turn to partition a dataset.  For this round we'll use data from Roberts et al. 2008 on bio-contaminants in Sydney Australia's Port Jackson Bay.  The data are measurements of metal content in two types of co-occurring algae at 10 sample sites around the bay.

```{r data}
#Read in data
metals_dat <- readr::read_csv(here::here("lab/data/Harbour_metals.csv"))

# Inspect the data
#View(metals_dat)

#Grab pollutant variables
metals_dat2 <- metals_dat[, 4:8] 
```
1. Start with k-means clustering - kmeans().  You can start with fviz_nbclust() to identify the best value of k. Then plot the model you obtain with the optimal value of k. 

```{r}
# Set seed
set.seed(123)
```

```{r}
# Find optimal number of clusters using elbow method
fviz_nbclust(metals_dat2, kmeans, method = "wss")

# Perform k-means clustering with 3 clusters
kmeans_result <- kmeans(metals_dat2, centers = 3, nstart = 25)

# Plot the clusters
fviz_cluster(kmeans_result, geom = "point", data = metals_dat2) + ggtitle("k=3")
```

Do you notice anything different about the spacing between clusters?  Why might this be?

[When we connect the outermost points in each cluster, we see that there is overlap between the far right side of cluster 2 and the far left side of cluster 3. The points near this boundary might have been assigned this way just because of the other nearby points that belonged to each cluster, or this could indicate that we should have chosen a larger number of clusters.]{style="color:navy;"}


Run summary() on your model object.  Does anything stand out?

```{r}
# Inspect model object
kmeans_result
```

[Looking at the specific values, 'size', which give the number of points allocated to each of the three clusters, stands out because cluster 2 and cluster 3 both have more than double the number of points as cluster 1. In addition, our 'withinss' values, which give the within-cluster variation for each of the three clusters, stands out because cluster 3 has over five times more variation than cluster 1 and over three times more variation than cluster 2.]{style="color:navy;"}

2. Good, now let's move to hierarchical clustering that we saw in lecture. The first step for that is to calculate a distance matrix on the data (using dist()). Euclidean is a good choice for the distance method.

```{r}
# Calculate distance matrix
dist_matrix <- dist(metals_dat2, method = "euclidean")
```


2. Use tidy() on the distance matrix so you can see what is going on. What does each row in the resulting table represent?

```{r}
tidy(dist_matrix)
```

[Each row of this table tells us the Euclidean distance between a set of two points in our data. There are 1770 rows in the table, one for each unique set of points.]{style="color:navy;"}

3. Then apply hierarchical clustering with hclust().

```{r}
# 3. Apply hierarchical clustering
hc_result <- hclust(dist_matrix)
```


4. Now plot the clustering object. You can use something of the form plot(as.dendrogram()).  Or you can check out the cool visual options here: https://rpubs.com/gaston/dendrograms

```{r}

# 4. Plot the clustering object
plot(as.dendrogram(hc_result), main = "Hierarchical Clustering Dendrogram")

```

How does the plot look? Do you see any outliers?  How can you tell?  

[The plot looks as expected. 51 is clearly an outlier point because starting from the bottom of the dendrogram and moving up, it is by far the last point to be assigned to a cluster that includes any other points besides itself, and when this assignment does occur, the point is about 80 distance units away from the centroid of the cluster it is assigned to. Comparatively, the point with the next highest distance away from the centroid of its initial assignment to a cluster containing more than just itself is 15, at about 40 distance units.]{style="color:navy;"}
